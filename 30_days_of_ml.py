# -*- coding: utf-8 -*-
"""30_days_of_ml.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_TtsZhgoRD1c0mkCtKrFQHCuNdb484D1
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sn

"""**LOADING THE DATASET**





"""

train_data=pd.read_csv('/content/drive/MyDrive/30-days-of-ml/train.csv')
test_data=pd.read_csv('/content/drive/MyDrive/30-days-of-ml/test.csv')
sample_submission=pd.read_csv('/content/drive/MyDrive/30-days-of-ml/sample_submission.csv')

train_data.head()

train_data.describe()

train_data.shape

"""creating kfold"""

from sklearn import model_selection
train_data["kfold"] = -1
kf = model_selection.KFold(n_splits=10, shuffle=True, random_state=42)
for fold, (train_indicies, valid_indicies) in enumerate(kf.split(X=train_data)):
    train_data.loc[valid_indicies, "kfold"] = fold


train_data.to_csv("train_folds.csv", index=False)

!pip install autoviz
!pip install xlrd

from autoviz.AutoViz_Class import AutoViz_Class
AV = AutoViz_Class()

auto_eda=ff = AV.AutoViz('/content/drive/MyDrive/30-days-of-ml/train.csv')

cat_cols = [col for col in train_data.columns if 'cat' in col]

from sklearn.preprocessing import OrdinalEncoder
ordinal_encoder = OrdinalEncoder()
train_data[cat_cols] = ordinal_encoder.fit_transform(train_data[cat_cols])
test_data[cat_cols]=ordinal_encoder.transform(test_data[cat_cols])

train_data.head()

# Separate target from predictors
y = train_data.target
X = train_data.drop(['id','target'], axis=1)

test_id=test_data['id']
test_data=test_data.drop(['id'], axis=1)

!pip install catboost

from sklearn.metrics import mean_squared_error
from sklearn.model_selection import KFold, cross_val_score
kf = KFold(n_splits=10, random_state=42, shuffle=True)
def cv_rmse(model):
    return -cross_val_score(model, X, y, scoring='neg_root_mean_squared_error', cv=kf)

from sklearn.linear_model import LinearRegression, ElasticNet, Lasso, Ridge
scores=[]
lin = LinearRegression()
score_lin = cv_rmse(lin)
scores.append(score_lin.mean())

R = Ridge()
score_R = cv_rmse(R)
scores.append(score_R.mean())

L = Lasso()
score_L = cv_rmse(L)
scores.append(score_L.mean())

EN = ElasticNet()
score_EN = cv_rmse(EN)
scores.append(score_EN.mean())

#from sklearn.ensemble import RandomForestRegressor
#RF = RandomForestRegressor()
#score_RF = cv_rmse(RF)
#scores.append(score_RF.mean())

from catboost import CatBoostRegressor
CB = CatBoostRegressor()
score_CB = cv_rmse(CB)
scores.append(score_CB.mean())

import xgboost as xg
xgb = xg.XGBRegressor()
score_xgb = cv_rmse(xgb)
scores.append(score_xgb.mean())

scores

from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
catM = CatBoostRegressor(verbose=0, allow_writing_files=False)
params = {'learning_rate': [0.1,0.2,0.3,0.4,0.5,0.01,0.05,0.005,0.0005],
              'depth': [4,5,7,8,9,6,10],
              'l2_leaf_reg': [1,2,3,4,5,6,7,8,9]}

grid_search_cat = RandomizedSearchCV(estimator=catM, scoring='neg_root_mean_squared_error', param_distributions=params, n_iter=10, cv=4, verbose=2,
                                     random_state=42, n_jobs=-1)
grid_search_cat.fit(X, y)
catModel = grid_search_cat.best_estimator_
print('Best params(CatBoost):',grid_search_cat.best_params_)
print('RMSE(CatBoost):', -grid_search_cat.best_score_)

"""model formation

"""

#catModel = catBoost(X_train, y_train, X_test)
predictions = {}
from catboost import CatBoostRegressor
catModel = CatBoostRegressor(verbose=0, allow_writing_files=False, learning_rate=0.15, l2_leaf_reg=2, depth=4)
catModel.fit(X, y)
predictions['CatBoost'] = catModel.predict(test_data)

sample_submission.head()

df = pd.DataFrame({'id': test_id, 'target': predictions['CatBoost']})

df.head()

df.to_csv('r_2.csv', index=False)
df.head()

from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
def catBoost(X_train, y_train, X_test):
    catM = CatBoostRegressor(verbose=0, allow_writing_files=False)
    params = {'learning_rate': [0.1,0.2,0.3,0.4,0.5,0.01,0.05,0.005,0.0005],
              'depth': [4,5,7,8,9,6,10],
              'l2_leaf_reg': [1,2,3,4,5,6,7,8,9]}

    grid_search_cat = RandomizedSearchCV(estimator=catM, scoring='neg_root_mean_squared_error', param_distributions=params, n_iter=10, cv=4, verbose=2,
                                     random_state=42, n_jobs=-1)
    grid_search_cat.fit(X_train, y_train)
    catModel = grid_search_cat.best_estimator_
    print('Best params(CatBoost):',grid_search_cat.best_params_)
    print('RMSE(CatBoost):', -grid_search_cat.best_score_)
    return catModel

predictions={}
import xgboost as xg
xgb = xg.XGBRegressor(learning_rate=0.2,max_depth=3,n_jobs=4,random_state=42,n_estimators=1000)
xgb.fit(X,y)
predictions['xgboost'] = xgb.predict(test_data)
df = pd.DataFrame({'id': test_id, 'target': predictions['xgboost']})
df.to_csv('r_4.csv', index=False)
df.head()

"""**modelling**"""

from xgboost import XGBRegressor
xgb_params = {
    'n_estimators': 10000,
    'learning_rate': 0.03628302216953097,
    'subsample': 0.7875490025178415,
    'colsample_bytree': 0.11807135201147481,
    'max_depth': 3,
    'booster': 'gbtree', 
    'reg_lambda': 0.0008746338866473539,
    'reg_alpha': 23.13181079976304,
    'random_state':1,
    'n_jobs': 4
}

useful_features = [c for c in train_data.columns if c not in ("id", "target", "kfold")]
object_cols = [col for col in useful_features if 'cat' in col]
test_data =test_data[useful_features]

"""kfold"""

from sklearn.metrics import mean_squared_error
from sklearn.model_selection import KFold
from sklearn.preprocessing import OrdinalEncoder
mean_rmse=0

final_predictions = []
for fold in range(5):
    xtrain =  train_data[train_data .kfold != fold].reset_index(drop=True)
    xvalid = train_data[train_data .kfold == fold].reset_index(drop=True)
    xtest = test_data.copy()

    ytrain = xtrain.target
    yvalid = xvalid.target
    
    xtrain = xtrain[useful_features]
    xvalid = xvalid[useful_features]

    ordinal_encoder = OrdinalEncoder()
    xtrain[object_cols] = ordinal_encoder.fit_transform(xtrain[object_cols])
    xvalid[object_cols] = ordinal_encoder.transform(xvalid[object_cols])
    xtest[object_cols] = ordinal_encoder.transform(xtest[object_cols])

    
    model = XGBRegressor(**xgb_params,tree_method='gpu_hist', gpu_id=0, predictor="gpu_predictor")
    model.fit(xtrain, ytrain)
    preds_valid = model.predict(xvalid)
    test_preds = model.predict(xtest)
    final_predictions.append(test_preds)
    print(f"fold: {fold}, rmse: {mean_squared_error(yvalid, preds_valid, squared=False)}")

#feature importance
importance = model.feature_importances_
# summarize feature importance
for i,v in enumerate(importance):
	print('Feature: %0d, Score: %.5f' % (i,v))
# plot feature importance
plt.bar([x for x in range(len(importance))], importance)
plt.show()

preds = np.mean(np.column_stack(final_predictions), axis=1)
sample_submission.target = preds
sample_submission.to_csv("xgb00st_submission_2.csv", index=False)

{'learning_rate': 0.016023728342443467,
 'reg_lambda': 8.516903774185477e-06,
 'reg_alpha': 3.503912313878963,
 'subsample': 0.7500229817441502,
 'colsample_bytree': 0.12623774319335374,
 'max_depth': 5,
 'min_child_weight': 1,
 'gamma': 0.5}

